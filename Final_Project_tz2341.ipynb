{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('movies.dat', header = None, sep='::', names =['MovieID', 'Title', 'Genres'],\n",
    "                    engine='python')\n",
    "users_df = pd.read_csv('users.dat',  header = None, sep='::', \n",
    "                    engine='python')\n",
    "ratings_df = pd.read_csv('ratings.dat',  header = None, sep='::', names = ['UserID', 'MovieID', 'Rating', 'Timestamp'],\n",
    "                    engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_df = ratings_df.pivot(index = 'UserID', columns ='MovieID', values = 'Rating').fillna(0)\n",
    "R = R_df.as_matrix()\n",
    "R_idx = np.argwhere(R!=0)\n",
    "R_train_idx, R_test_idx = train_test_split(R_idx, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train = R.copy()\n",
    "R_test = R.copy()\n",
    "R_train[R_test_idx[:, 0], R_test_idx[:, 1]] = 0\n",
    "R_test[R_train_idx[:, 0], R_train_idx[:, 1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 3.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 3.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_test_split = 0.9\n",
    "# U, I  = np.shape(R)\n",
    "# R_train = copy.deepcopy(R)\n",
    "# R_train[:,int(I*train_test_split):] = 0\n",
    "# R_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def r_hat (mu, P, Q, bu, bi):\n",
    "    temp = np.dot(P, Q.T) + mu\n",
    "    temp_u = np.array([(x + bu.T).ravel() for x in temp.T]).T\n",
    "    return np.array([(x + bi.T).ravel() for x in temp_u])\n",
    "\n",
    "def loss_function(R, P, Q, F, bu, bi, beta):\n",
    "    e = 0\n",
    "    N = np.count_nonzero(R)\n",
    "    mu = np.sum(np.sum(R, axis = 1)) / N\n",
    "    test_idx = R == 0\n",
    "    R_hat = r_hat(mu, P, Q, bu, bi)\n",
    "    R_hat[test_idx] = 0\n",
    "    # loss function error sum( (y-y_hat)^2 )\n",
    "    error = (R - R_hat)**2\n",
    "    e = sum(np.sum(error, axis = 1))\n",
    "    if F > 0:\n",
    "        e += beta/2.0 *(sum([sum(n * p) for n, p in zip(np.count_nonzero(error, axis = 1), P)]))\n",
    "        e += beta/2.0 *(sum([sum(n * q) for n, q in zip(np.count_nonzero(error, axis = 0), Q)]))\n",
    "#     regularization = np.dot (P**2, (Q**2).T)\n",
    "#     regularization[test_idx] = 0\n",
    "#     e += beta/2.0*(sum(np.sum(regularization, axis = 1)))\n",
    "    return e/N\n",
    "\n",
    "def loss_gradient(R, P, Q, F, bu, bi, learning_rate=0.0001, penalty = 0.02):\n",
    "    N = np.count_nonzero(R)\n",
    "    mu = np.sum(np.sum(R, axis = 1)) / N\n",
    "    test_idx = R == 0\n",
    "    R_hat = r_hat(mu, P, Q, bu, bi)\n",
    "    R_hat[test_idx] = 0\n",
    "    e = (R - R_hat)\n",
    "    \n",
    "    grad_bu = - np.sum(e,axis = 1).reshape(-1,1)\n",
    "    grad_bi = - np.sum(e, axis = 0).reshape(-1,1)\n",
    "    \n",
    "    grad_P = np.zeros((len(P), F))\n",
    "    grad_Q = np.zeros((len(Q), F))\n",
    "    if F > 0:\n",
    "#         grad_P = np.array([np.array([-np.dot(r, q) for q in Q.T]) + penalty * p for r, p in zip(e, P)])\n",
    "#         grad_Q = np.array([np.array([-np.dot(r, p) for p in P.T]) + penalty * q for r, q in zip(e.T, Q)])\n",
    "        grad_P = - np.dot(e, Q) + penalty * P\n",
    "        grad_Q = - np.dot(e.T, P) + penalty * Q\n",
    "    \n",
    "#     if F > 0:\n",
    "#         train_idx = np.argwhere(e!=0)\n",
    "#         for idx in train_idx:\n",
    "#             u, i = idx\n",
    "#             P[u] += learning_rate*(e[u, i]*Q[i] - penalty*P[u])\n",
    "#             Q[i] += learning_rate*(e[u, i]*P[u] - penalty*Q[i])\n",
    "    return grad_P, grad_Q, grad_bu, grad_bi\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_function(e, R, P, Q, F, bu, bi, beta):\n",
    "    U = R.shape[0]\n",
    "    perm = np.random.choice(R.shape[0],U)\n",
    "    loss = loss_function(R[perm], P[perm], Q, F, bu[perm], bi, beta)\n",
    "    N = np.count_nonzero(R[perm])\n",
    "    mu = np.sum(np.sum(R[perm], axis = 1)) / N\n",
    "    idx = R[perm] == 0\n",
    "    l = r_hat(mu, P[perm], Q, bu[perm], bi)\n",
    "    l[idx] = 0\n",
    "    err=(l-R[perm])**2\n",
    "    l[idx] = 1\n",
    "    err/=l\n",
    "    train_dispersion=np.mean(np.mean(err, axis = 1))\n",
    "    print(\"\\t\",e,\"Loss =\",loss,\"Train_Dispersion\",train_dispersion,end=\"\\t\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def optimization_SGD(R, P, Q, F, bu, bi, \n",
    "                       learning_rate=0.0001,\n",
    "                       penalty = 0.02,\n",
    "                       tol=1e-8, \n",
    "                       max_iter=1000, \n",
    "                       batch_size=100,\n",
    "                       verbose = True):\n",
    "    N = R.shape[0]\n",
    "    l0 = loss_function(R, P, Q, F, bu, bi, penalty)\n",
    "    \n",
    "    for e in range(max_iter):\n",
    "        if (e%(max_iter//10)==0 and verbose): ##train a small batches of data\n",
    "            report_function(e, R, P, Q, F, bu, bi, penalty)\n",
    "        perm = np.random.permutation(N)\n",
    "        for i in range(0,N,batch_size):\n",
    "            Rb = R[perm[i:i+batch_size]]\n",
    "            Pb = P[perm[i:i+batch_size]]\n",
    "            bub = bu[perm[i:i+batch_size]]\n",
    "#             print (np.shape(P[perm[i:i+batch_size]]), np.shape(Q),np.shape(bub),np.shape(bi))\n",
    "            grad_P, grad_Q, grad_bu, grad_bi = loss_gradient(Rb, Pb, Q, F, bub, bi, learning_rate, penalty)\n",
    "            \n",
    "            #update P, Q, bu, bi\n",
    "            P[perm[i:i+batch_size]] -= learning_rate*grad_P\n",
    "            bu[perm[i:i+batch_size]] -= learning_rate*grad_bu\n",
    "            \n",
    "            Q -= learning_rate*grad_Q\n",
    "            bi -= learning_rate*grad_bi\n",
    "            \n",
    "        l = loss_function(R, P, Q, F, bu, bi, penalty)\n",
    "       \n",
    "        d=np.abs(l-l0)\n",
    "        if d<tol*l0:\n",
    "            break\n",
    "        l0=l  \n",
    "    if verbose:\n",
    "        report_function(e, R, P, Q, F, bu, bi, penalty)\n",
    "        \n",
    "    return P, Q, bu, bi\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecommenderModel(object):\n",
    "    def __init__(self, \n",
    "                 F = 0,\n",
    "                 learning_rate=0.001,\n",
    "                 penalty = 0.02,\n",
    "                 tol=1e-8, \n",
    "                 max_iter=1000, \n",
    "                 batch_size=100,\n",
    "                 verbose = True):\n",
    "        self.mu = None\n",
    "        self.Q = None\n",
    "        self.P = None\n",
    "        self.bu = None\n",
    "        self.bi = None\n",
    "        \n",
    "        ## initialize hyper parameters\n",
    "        self.F = F\n",
    "        self.learning_rate = learning_rate\n",
    "        self.penalty = penalty\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    \n",
    "    def fit_matrix_factorization_model(self, R):\n",
    "        \"\"\"\n",
    "        Return the fitted model\n",
    "        @param: matrix factor\n",
    "        \"\"\"\n",
    "        U = len(R)\n",
    "        I = len(R[0])   \n",
    "        F = self.F\n",
    "        \n",
    "        ## Generate user bias bu U coefficient\n",
    "        ## Generte item bias bi I coefficient\n",
    "        bu = np.random.normal(0, 1e-4, (U, 1))\n",
    "        bi = np.random.normal(0, 1e-4, (I, 1))\n",
    "        \n",
    "        # Generate P - U x F\n",
    "        # Use random values to start\n",
    "        P = np.random.normal(0, 1/max(1, F**0.5), (U, F))\n",
    "\n",
    "        # Generate Q - I x F\n",
    "        # Use random values to start\n",
    "        Q = np.random.normal(0, 1/max(1, F**0.5), (I, F))\n",
    "        \n",
    "        # Calculate mu \n",
    "        N = np.count_nonzero(R)\n",
    "        self.mu = np.sum(np.sum(R, axis = 1)) / N\n",
    "\n",
    "        P, Q, bu, bi = optimization_SGD(R, P, Q, F, bu, bi, \n",
    "                                        self.learning_rate, \n",
    "                                        self.penalty,\n",
    "                                        self.tol,\n",
    "                                        self.max_iter,\n",
    "                                        self.batch_size,\n",
    "                                        self.verbose)\n",
    "\n",
    "        self.Q = Q\n",
    "        self.P = P\n",
    "        self.bu = bu\n",
    "        self.bi = bi\n",
    "        return self\n",
    "        \n",
    "    def predict_instance(self, p_index):\n",
    "        \"\"\"\n",
    "        Returns all predictions for a given row\n",
    "        @param p_index:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        return self.mu + self.bu[p_index] + self.bi.T + np.dot(self.P[p_index, :], self.Q.T)\n",
    "\n",
    "    def predict_all(self):\n",
    "        \"\"\"\n",
    "        @Returns the full prediction matrix\n",
    "        @param:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        temp = np.dot(self.P, self.Q.T) + self.mu\n",
    "        temp_u = np.array([(x + self.bu.T).ravel() for x in temp.T]).T\n",
    "        return np.array([(x + self.bi.T).ravel() for x in temp_u])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = np.arange(0.0001, 0.001, 0.0004)\n",
    "F = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0 Loss = 1.24437397182 Train_Dispersion 0.012478633477\t\n",
      "\t 50 Loss = 0.880062970547 Train_Dispersion 0.00913554662111\t\n",
      "\t 100 Loss = 0.833213426462 Train_Dispersion 0.00861943946355\t\n",
      "\t 150 Loss = 0.831187109973 Train_Dispersion 0.00868988075443\t\n",
      "\t 200 Loss = 0.816706659024 Train_Dispersion 0.00865093217561\t\n",
      "\t 250 Loss = 0.815886150747 Train_Dispersion 0.00856140611781\t\n",
      "\t 300 Loss = 0.80889776851 Train_Dispersion 0.00858964528319\t\n",
      "\t 350 Loss = 0.814526861521 Train_Dispersion 0.00857552641327\t\n",
      "\t 371 Loss = 0.818722688218 Train_Dispersion 0.00854060188511\t\n",
      "train error (learning_rate = 0.0001, F = 0):  0.809778006563\n",
      "test error (learning_rate = 0.0001, F = 0):  0.823978880617\n",
      "\t 0 Loss = 1.2352371647 Train_Dispersion 0.0125277283037\t\n",
      "\t 50 Loss = 0.82118009325 Train_Dispersion 0.00870655718559\t\n",
      "\t 72 Loss = 0.823241082419 Train_Dispersion 0.00885311381979\t\n",
      "train error (learning_rate = 0.0005, F = 0):  0.810287865905\n",
      "test error (learning_rate = 0.0005, F = 0):  0.824479256409\n",
      "\t 0 Loss = 1.26062431503 Train_Dispersion 0.0126386067318\t\n",
      "\t 39 Loss = 0.815678131861 Train_Dispersion 0.00861283667714\t\n",
      "train error (learning_rate = 0.0009000000000000001, F = 0):  0.810868286915\n",
      "test error (learning_rate = 0.0009000000000000001, F = 0):  0.825195540061\n"
     ]
    }
   ],
   "source": [
    "train_error = []\n",
    "test_error = []\n",
    "for learning_rate in learning_rates:\n",
    "    model = RecommenderModel(learning_rate = learning_rate, tol=1e-5, max_iter = 500, F = F, batch_size = 100, penalty = 0.02)\n",
    "    model.fit_matrix_factorization_model(R_train)\n",
    "    test_idx = R == 0\n",
    "    R_hat = model.predict_all()\n",
    "    R_hat[test_idx] = 0\n",
    "    all_error = (R - R_hat)**2\n",
    "    train_error += [np.sum(all_error[R_train_idx[:, 0], R_train_idx[:, 1]])/ len(R_train_idx)]         \n",
    "    test_error += [np.sum(all_error[R_test_idx[:, 0], R_test_idx[:, 1]])/ len(R_test_idx)]   \n",
    "    print (\"train error (learning_rate = {0}, F = {1}): \".format(learning_rate, F), train_error[-1])\n",
    "    print (\"test error (learning_rate = {0}, F = {1}): \".format(learning_rate, F), test_error[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best train error (penalty = 0.0001, F = 0):  0.809778006563\n",
      "best test error (penalty = 0.0001, F = 0):  0.823978880617\n"
     ]
    }
   ],
   "source": [
    "best_idx = np.argmin(test_error)\n",
    "best_learning_rate = learning_rates[best_idx]\n",
    "print (\"best train error (penalty = {0}, F = {1}): \".format(best_learning_rate, F), train_error[best_idx])\n",
    "print (\"best test error (penalty = {0}, F = {1}): \".format(best_learning_rate, F), test_error[best_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf=KFold(5,shuffle=True)\n",
    "folds=list(kf.split(R_train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_cross_validation(model, R, folds):\n",
    "    kfolds=len(folds)\n",
    "    train_performance=np.empty(kfolds)\n",
    "    validation_performance=np.empty(kfolds)\n",
    "    for idx in range(kfolds):\n",
    "        train_idx,validation_idx=folds[idx]\n",
    "        R_train_cv = R_train.copy()\n",
    "        R_val_cv = R_train.copy()\n",
    "        R_train_cv[R_train_idx[validation_idx][:,0], R_train_idx[validation_idx][:,1]] = 0\n",
    "        R_val_cv[R_train_idx[train_idx][:,0], R_train_idx[train_idx][:,1]] = 0\n",
    "        N_train = len(train_idx)\n",
    "        N_val = len(validation_idx)\n",
    "        ##fit the model\n",
    "        model.fit_matrix_factorization_model(R_train_cv)\n",
    "        test_idx = R_train == 0\n",
    "        R_hat_cv = model.predict_all()  \n",
    "        R_hat_cv[test_idx] = 0\n",
    "        all_error = (R_train - R_hat_cv)**2    \n",
    "        train_error_curr = np.sum(all_error[R_train_idx[train_idx][:,0], R_train_idx[train_idx][:,1]])/N_train       \n",
    "        validation_error_curr = np.sum(all_error[R_train_idx[validation_idx][:,0], R_train_idx[validation_idx][:,1]])/N_val\n",
    "        train_performance[idx] = train_error_curr\n",
    "        validation_performance[idx] = validation_error_curr\n",
    "    return np.array(train_performance),np.array(validation_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0 Loss = 1.74107824236 Train_Dispersion 0.0159397890023\t\n",
      "\t 50 Loss = 0.96040523319 Train_Dispersion 0.00825362681046\t\n",
      "\t 100 Loss = 0.876979948411 Train_Dispersion 0.00748258249428\t\n",
      "\t 150 Loss = 0.844405399665 Train_Dispersion 0.00712810119156\t\n",
      "\t 200 Loss = 0.823292802073 Train_Dispersion 0.0069247151007\t\n",
      "\t 250 Loss = 0.82236151957 Train_Dispersion 0.00688299874038\t\n",
      "\t 300 Loss = 0.818463163773 Train_Dispersion 0.00694437306877\t\n",
      "\t 350 Loss = 0.813491117228 Train_Dispersion 0.00685812871573\t\n",
      "\t 400 Loss = 0.811205344055 Train_Dispersion 0.00683173480101\t\n",
      "\t 450 Loss = 0.798905658648 Train_Dispersion 0.00659912677241\t\n",
      "\t 499 Loss = 0.791467321336 Train_Dispersion 0.00688537541793\t\n",
      "\t 0 Loss = 1.75394272854 Train_Dispersion 0.0175465414227\t\n",
      "\t 50 Loss = 0.953263477503 Train_Dispersion 0.00820843110221\t\n",
      "\t 100 Loss = 0.867261448125 Train_Dispersion 0.0073829723417\t\n",
      "\t 150 Loss = 0.844989728499 Train_Dispersion 0.00725442351552\t\n",
      "\t 200 Loss = 0.833938044061 Train_Dispersion 0.00699173716763\t\n",
      "\t 250 Loss = 0.822519454486 Train_Dispersion 0.00687578356571\t\n",
      "\t 300 Loss = 0.821219281446 Train_Dispersion 0.00708317768292\t\n",
      "\t 350 Loss = 0.810707357264 Train_Dispersion 0.00692241489532\t\n",
      "\t 400 Loss = 0.812267068558 Train_Dispersion 0.00688770797165\t\n",
      "\t 450 Loss = 0.808685035754 Train_Dispersion 0.00696631758391\t\n",
      "\t 499 Loss = 0.79974754715 Train_Dispersion 0.00672610879793\t\n",
      "\t 0 Loss = 1.73273904656 Train_Dispersion 0.0298865629484\t\n",
      "\t 50 Loss = 0.952314689788 Train_Dispersion 0.00798477950442\t\n",
      "\t 100 Loss = 0.884293922798 Train_Dispersion 0.00734230735137\t\n",
      "\t 150 Loss = 0.848027524501 Train_Dispersion 0.00717560705436\t\n",
      "\t 200 Loss = 0.835373048814 Train_Dispersion 0.00721674754584\t\n",
      "\t 250 Loss = 0.824003120394 Train_Dispersion 0.00694932738235\t\n",
      "\t 300 Loss = 0.816095113351 Train_Dispersion 0.00692566356084\t\n",
      "\t 350 Loss = 0.79971867759 Train_Dispersion 0.00664091615587\t\n",
      "\t 400 Loss = 0.804451975038 Train_Dispersion 0.00663329676171\t\n",
      "\t 450 Loss = 0.810043410298 Train_Dispersion 0.00704336385141\t\n",
      "\t 499 Loss = 0.801391947852 Train_Dispersion 0.00682004439778\t\n",
      "\t 0 Loss = 1.73750753896 Train_Dispersion 0.0159401988502\t\n",
      "\t 50 Loss = 0.966279228331 Train_Dispersion 0.00824595579266\t\n",
      "\t 100 Loss = 0.863492836428 Train_Dispersion 0.00741529151522\t\n",
      "\t 150 Loss = 0.852534401842 Train_Dispersion 0.00705986146571\t\n",
      "\t 200 Loss = 0.827471616635 Train_Dispersion 0.00694586425223\t\n",
      "\t 250 Loss = 0.826505931881 Train_Dispersion 0.00681238352842\t\n",
      "\t 300 Loss = 0.808204192738 Train_Dispersion 0.00688709151245\t\n",
      "\t 350 Loss = 0.807639374015 Train_Dispersion 0.00698243691968\t\n",
      "\t 400 Loss = 0.797538686146 Train_Dispersion 0.00677935595055\t\n",
      "\t 450 Loss = 0.797637917489 Train_Dispersion 0.00669860183311\t\n",
      "\t 499 Loss = 0.788730998791 Train_Dispersion 0.00661827243506\t\n",
      "\t 0 Loss = 1.74721255104 Train_Dispersion 0.0175232097692\t\n",
      "\t 50 Loss = 0.944993928837 Train_Dispersion 0.00800142065125\t\n",
      "\t 100 Loss = 0.874073034028 Train_Dispersion 0.00752347009684\t\n",
      "\t 150 Loss = 0.853406799344 Train_Dispersion 0.0072314589133\t\n",
      "\t 200 Loss = 0.822058308535 Train_Dispersion 0.00701040669107\t\n",
      "\t 250 Loss = 0.825381013601 Train_Dispersion 0.00697411586376\t\n",
      "\t 300 Loss = 0.826808029889 Train_Dispersion 0.00733789434451\t\n",
      "\t 350 Loss = 0.820579505402 Train_Dispersion 0.00703749114152\t\n",
      "\t 400 Loss = 0.802338930652 Train_Dispersion 0.0068194570964\t\n",
      "\t 450 Loss = 0.807564433244 Train_Dispersion 0.00707301043495\t\n",
      "\t 499 Loss = 0.81189706947 Train_Dispersion 0.00697337632003\t\n",
      "----------------------------------------------------------------------------------\n",
      "F=2, learning_rate = 0.0001, penalty=0.02, max_iter = 500, batch_size = 100 :\n",
      "0.801090400604 0.840473247236 0.0012894680075 0.00351527791594\n",
      "==================================================================================\n",
      "\t 0 Loss = 1.50567875053 Train_Dispersion 0.0134816895297\t\n",
      "\t 50 Loss = 0.95498329136 Train_Dispersion 0.00795040747689\t\n",
      "\t 100 Loss = 0.866643085908 Train_Dispersion 0.00747879597574\t\n",
      "\t 150 Loss = 0.844408230226 Train_Dispersion 0.00725811037627\t\n",
      "\t 200 Loss = 0.826505704836 Train_Dispersion 0.00712773728384\t\n",
      "\t 250 Loss = 0.823877323352 Train_Dispersion 0.00699840348815\t\n",
      "\t 300 Loss = 0.817824599924 Train_Dispersion 0.00685709612403\t\n",
      "\t 350 Loss = 0.801750032212 Train_Dispersion 0.00682819277687\t\n",
      "\t 400 Loss = 0.794900699918 Train_Dispersion 0.00681983771275\t\n",
      "\t 450 Loss = 0.788078450138 Train_Dispersion 0.00681199936726\t\n",
      "\t 499 Loss = 0.799901073971 Train_Dispersion 0.00665124004046\t\n",
      "\t 0 Loss = 1.49474784313 Train_Dispersion 0.0120829243877\t\n",
      "\t 50 Loss = 0.951100625269 Train_Dispersion 0.00782036257618\t\n",
      "\t 100 Loss = 0.86846845503 Train_Dispersion 0.00725966364779\t\n",
      "\t 150 Loss = 0.837543232036 Train_Dispersion 0.00710557969332\t\n",
      "\t 200 Loss = 0.829750501428 Train_Dispersion 0.00683878094871\t\n",
      "\t 250 Loss = 0.813522317567 Train_Dispersion 0.00689133564784\t\n",
      "\t 300 Loss = 0.802363083203 Train_Dispersion 0.00708460126445\t\n",
      "\t 350 Loss = 0.8030244816 Train_Dispersion 0.00686302634036\t\n",
      "\t 400 Loss = 0.792320569667 Train_Dispersion 0.00668643993196\t\n",
      "\t 450 Loss = 0.799823501913 Train_Dispersion 0.00687958443548\t\n",
      "\t 499 Loss = 0.782819715481 Train_Dispersion 0.0066433822506\t\n",
      "\t 0 Loss = 1.48671072122 Train_Dispersion 0.0119434646623\t\n",
      "\t 50 Loss = 0.936453695381 Train_Dispersion 0.00777504986906\t\n",
      "\t 100 Loss = 0.867275004418 Train_Dispersion 0.00716581729758\t\n",
      "\t 150 Loss = 0.844377058011 Train_Dispersion 0.00714702605387\t\n",
      "\t 200 Loss = 0.835706856792 Train_Dispersion 0.00710573267422\t\n",
      "\t 250 Loss = 0.812913175504 Train_Dispersion 0.00707233969651\t\n",
      "\t 300 Loss = 0.803726908288 Train_Dispersion 0.00687004328152\t\n",
      "\t 350 Loss = 0.80256936482 Train_Dispersion 0.00680980836011\t\n",
      "\t 400 Loss = 0.809248616611 Train_Dispersion 0.00688749143454\t\n",
      "\t 450 Loss = 0.799760573201 Train_Dispersion 0.00683891189267\t\n",
      "\t 499 Loss = 0.79271894148 Train_Dispersion 0.00670247958613\t\n",
      "\t 0 Loss = 1.50307915329 Train_Dispersion 0.0124138903596\t\n",
      "\t 50 Loss = 0.953750421364 Train_Dispersion 0.00805483515533\t\n",
      "\t 100 Loss = 0.871710616206 Train_Dispersion 0.00744259964595\t\n",
      "\t 150 Loss = 0.84914343808 Train_Dispersion 0.00713163511727\t\n",
      "\t 200 Loss = 0.825462062656 Train_Dispersion 0.00691284708578\t\n",
      "\t 250 Loss = 0.815839513647 Train_Dispersion 0.00675920802238\t\n",
      "\t 300 Loss = 0.808819836293 Train_Dispersion 0.00687390414173\t\n",
      "\t 350 Loss = 0.810620746155 Train_Dispersion 0.00694594329826\t\n",
      "\t 400 Loss = 0.797864757133 Train_Dispersion 0.00674283375045\t\n",
      "\t 450 Loss = 0.799076203669 Train_Dispersion 0.00681729648559\t\n",
      "\t 499 Loss = 0.792352050441 Train_Dispersion 0.00662059349308\t\n",
      "\t 0 Loss = 1.47220588904 Train_Dispersion 0.0120617368607\t\n",
      "\t 50 Loss = 0.941164182634 Train_Dispersion 0.00793302153209\t\n",
      "\t 100 Loss = 0.868659750682 Train_Dispersion 0.00723977724658\t\n",
      "\t 150 Loss = 0.844704420708 Train_Dispersion 0.00727317672598\t\n",
      "\t 200 Loss = 0.826027086543 Train_Dispersion 0.00707791078765\t\n",
      "\t 250 Loss = 0.798047763027 Train_Dispersion 0.00691113012113\t\n",
      "\t 300 Loss = 0.810127644688 Train_Dispersion 0.00711472464285\t\n",
      "\t 350 Loss = 0.805109566471 Train_Dispersion 0.00697943839672\t\n",
      "\t 400 Loss = 0.794821656276 Train_Dispersion 0.00677847654796\t\n",
      "\t 450 Loss = 0.799567065659 Train_Dispersion 0.00688419187104\t\n",
      "\t 499 Loss = 0.798676375444 Train_Dispersion 0.00692113630061\t\n",
      "----------------------------------------------------------------------------------\n",
      "F=4, learning_rate = 0.0001, penalty=0.04, max_iter = 500, batch_size = 500 :\n",
      "0.792167747087 0.845025166979 0.0013307344467 0.0028059232659\n",
      "==================================================================================\n",
      "\t 0 Loss = 1.41827961247 Train_Dispersion 0.0112939142993\t\n",
      "\t 100 Loss = 0.870511307907 Train_Dispersion 0.00742110695443\t\n",
      "\t 200 Loss = 0.818641279571 Train_Dispersion 0.00701294777376\t\n",
      "\t 300 Loss = 0.797353471092 Train_Dispersion 0.00678360295373\t\n",
      "\t 400 Loss = 0.785234982843 Train_Dispersion 0.00656365190195\t\n",
      "\t 500 Loss = 0.775393344401 Train_Dispersion 0.00647524885098\t\n",
      "\t 600 Loss = 0.768986654472 Train_Dispersion 0.00648996976314\t\n",
      "\t 700 Loss = 0.759690401251 Train_Dispersion 0.00643840923621\t\n",
      "\t 800 Loss = 0.748995511794 Train_Dispersion 0.0064203609507\t\n",
      "\t 900 Loss = 0.733741128685 Train_Dispersion 0.00629085048388\t\n",
      "\t 999 Loss = 0.735120160429 Train_Dispersion 0.00631321232947\t\n",
      "\t 0 Loss = 1.40731614647 Train_Dispersion 0.0112741448774\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 100 Loss = 0.868328006106 Train_Dispersion 0.00723880082685\t\n",
      "\t 200 Loss = 0.82273295042 Train_Dispersion 0.00687175987892\t\n",
      "\t 300 Loss = 0.806342913475 Train_Dispersion 0.00681930900505\t\n",
      "\t 400 Loss = 0.805986459603 Train_Dispersion 0.00668889998375\t\n",
      "\t 500 Loss = 0.781538126983 Train_Dispersion 0.00676281157289\t\n",
      "\t 600 Loss = 0.773911477593 Train_Dispersion 0.00651475191818\t\n",
      "\t 700 Loss = 0.771675596839 Train_Dispersion 0.0066571433895\t\n",
      "\t 800 Loss = 0.752022869387 Train_Dispersion 0.00655782192134\t\n",
      "\t 900 Loss = 0.750370516525 Train_Dispersion 0.00634626112218\t\n",
      "\t 999 Loss = 0.7353540898 Train_Dispersion 0.0063389662454\t\n",
      "\t 0 Loss = 1.42459499338 Train_Dispersion 0.0114108192863\t\n",
      "\t 100 Loss = 0.878947398515 Train_Dispersion 0.00745350716547\t\n",
      "\t 200 Loss = 0.824187953877 Train_Dispersion 0.00708111074123\t\n",
      "\t 300 Loss = 0.811690815986 Train_Dispersion 0.00708354051436\t\n",
      "\t 400 Loss = 0.788077128965 Train_Dispersion 0.00662902875988\t\n",
      "\t 500 Loss = 0.782568029665 Train_Dispersion 0.00661822507397\t\n",
      "\t 600 Loss = 0.777725504803 Train_Dispersion 0.00659428372884\t\n",
      "\t 700 Loss = 0.761064973519 Train_Dispersion 0.00641351774999\t\n",
      "\t 800 Loss = 0.75480068948 Train_Dispersion 0.00647301853957\t\n",
      "\t 900 Loss = 0.741154131436 Train_Dispersion 0.00623634413447\t\n",
      "\t 999 Loss = 0.725012804523 Train_Dispersion 0.00617866599219\t\n",
      "\t 0 Loss = 1.41954213982 Train_Dispersion 0.011657946053\t\n",
      "\t 100 Loss = 0.881242100327 Train_Dispersion 0.00732536562232\t\n",
      "\t 200 Loss = 0.824625605205 Train_Dispersion 0.00700959318552\t\n",
      "\t 300 Loss = 0.811673561444 Train_Dispersion 0.00689875366389\t\n",
      "\t 400 Loss = 0.796726720741 Train_Dispersion 0.00670890208933\t\n",
      "\t 500 Loss = 0.78411735902 Train_Dispersion 0.00660478629712\t\n",
      "\t 600 Loss = 0.779120575019 Train_Dispersion 0.00674658198636\t\n",
      "\t 700 Loss = 0.761616439105 Train_Dispersion 0.00643873230386\t\n",
      "\t 800 Loss = 0.758411862642 Train_Dispersion 0.00643743732279\t\n",
      "\t 900 Loss = 0.746159576368 Train_Dispersion 0.00632744695801\t\n",
      "\t 999 Loss = 0.744424322749 Train_Dispersion 0.00638204153374\t\n",
      "\t 0 Loss = 1.4180695493 Train_Dispersion 0.011495947514\t\n",
      "\t 100 Loss = 0.867608941807 Train_Dispersion 0.00710875958999\t\n",
      "\t 200 Loss = 0.818187025471 Train_Dispersion 0.00690090551873\t\n",
      "\t 300 Loss = 0.795010527263 Train_Dispersion 0.00674877708471\t\n",
      "\t 400 Loss = 0.790397476053 Train_Dispersion 0.00678524950705\t\n",
      "\t 500 Loss = 0.776053971778 Train_Dispersion 0.00682425062906\t\n",
      "\t 600 Loss = 0.772829947429 Train_Dispersion 0.00652303086538\t\n",
      "\t 700 Loss = 0.751083621471 Train_Dispersion 0.00649490257459\t\n",
      "\t 800 Loss = 0.746913201533 Train_Dispersion 0.00638137419216\t\n",
      "\t 900 Loss = 0.73733564771 Train_Dispersion 0.00631143132666\t\n",
      "\t 999 Loss = 0.721972890552 Train_Dispersion 0.00591303067753\t\n",
      "----------------------------------------------------------------------------------\n",
      "F=6, learning_rate = 0.0001, penalty=0.1, max_iter = 1000, batch_size = 1000 :\n",
      "0.729383912107 0.825782570518 0.00420925719894 0.00537646153488\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "train=[]\n",
    "train_std=[]\n",
    "validation=[]\n",
    "validation_std=[]\n",
    "models_params = [[2, best_learning_rate, 0.02, 1e-5, 500, 100],[4, best_learning_rate, 0.04, 1e-5, 500, 500],[6, best_learning_rate, 0.1, 1e-5, 1000, 1000]]\n",
    "models = []\n",
    "for i in range(len(models_params)):\n",
    "    F,learning_rate, penalty, tol, max_iter, batch_size=models_params[i]\n",
    "    models += [RecommenderModel(F,learning_rate, penalty, tol, max_iter, batch_size)]\n",
    "    t,v = model_cross_validation(models[i], R_train, folds)\n",
    "    train.append(t.mean())\n",
    "    train_std.append(t.std())\n",
    "    validation.append(v.mean())\n",
    "    validation_std.append(v.std())\n",
    "    print (\"----------------------------------------------------------------------------------\")\n",
    "    print(\"F={0}, learning_rate = {1}, penalty={2}, max_iter = {3}, batch_size = {4} :\".format(F, learning_rate, penalty, max_iter, batch_size))\n",
    "    print(t.mean(),v.mean(),t.std(),v.std())\n",
    "    print (\"==================================================================================\")\n",
    "validation=np.array(validation)\n",
    "train=np.array(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0 Loss = 1.40229490187 Train_Dispersion 0.0141176405261\t\n",
      "\t 100 Loss = 0.857359090466 Train_Dispersion 0.00898123379792\t\n",
      "\t 200 Loss = 0.808959173562 Train_Dispersion 0.00876305658539\t\n",
      "\t 300 Loss = 0.800951060107 Train_Dispersion 0.00815417343371\t\n",
      "\t 400 Loss = 0.777728966391 Train_Dispersion 0.00832153337415\t\n",
      "\t 500 Loss = 0.761345788573 Train_Dispersion 0.00829499159712\t\n",
      "\t 600 Loss = 0.755177612104 Train_Dispersion 0.00830584901377\t\n",
      "\t 700 Loss = 0.735881066941 Train_Dispersion 0.00793664494536\t\n",
      "\t 800 Loss = 0.727155364678 Train_Dispersion 0.00768154584249\t\n",
      "\t 900 Loss = 0.715223609737 Train_Dispersion 0.00790154388885\t\n",
      "\t 999 Loss = 0.710000580562 Train_Dispersion 0.00774103340954\t\n",
      "best model hyperparamters: \n",
      "F=6, learning_rate = 0.0001, penalty=0.1, max_iter = 1000, batch_size = 1000 :\n",
      "\n",
      "train error (learning_rate = 0.0001, F = 6):  0.810868286915\n",
      "test error (learning_rate = 0.0001, F = 6):  0.825195540061\n"
     ]
    }
   ],
   "source": [
    "best_model_idx = np.argmin(validation)\n",
    "best_model = models[best_model_idx]\n",
    "best_model.fit_matrix_factorization_model(R_train)\n",
    "best_R_hat = best_model.predict_all()\n",
    "best_R_hat[test_idx] = 0\n",
    "best_all_error = (R - best_R_hat)**2\n",
    "best_train_error = np.sum(best_all_error[R_train_idx[:, 0], R_train_idx[:,1]])/ len(R_train_idx)\n",
    "best_test_error = np.sum(best_all_error[R_test_idx[:, 0], R_test_idx[:,1]])/ len(R_test_idx)\n",
    "best_F,best_learning_rate, best_penalty, best_tol, best_max_iter, best_batch_size=models_params[best_model_idx]\n",
    "print(\"best model hyperparamters: \")\n",
    "print(\"F={0}, learning_rate = {1}, penalty={2}, max_iter = {3}, batch_size = {4} :\".format(best_F, best_learning_rate, best_penalty, best_max_iter, best_batch_size))\n",
    "print()\n",
    "print (\"train error (learning_rate = {0}, F = {1}): \".format(learning_rate, F), train_error[-1])\n",
    "print (\"test error (learning_rate = {0}, F = {1}): \".format(learning_rate, F), test_error[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
